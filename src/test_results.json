[
  {
    "source": "arxiv",
    "id": "2306.05212v1",
    "title": "RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit",
    "authors": [
      "Jiongnan Liu",
      "Jiajie Jin",
      "Zihan Wang",
      "Jiehan Cheng",
      "Zhicheng Dou",
      "Ji-Rong Wen"
    ],
    "abstract": "Although Large Language Models (LLMs) have demonstrated extraordinary capabilities in many domains, they still have a tendency to hallucinate and generate fictitious responses to user requests. This problem can be alleviated by augmenting LLMs with information retrieval (IR) systems (also known as retrieval-augmented LLMs). Applying this strategy, LLMs can generate more factual texts in response to user input according to the relevant content retrieved by IR systems from external corpora as references. In addition, by incorporating external knowledge, retrieval-augmented LLMs can answer in-domain questions that cannot be answered by solely relying on the world knowledge stored in parameters. To support research in this area and facilitate the development of retrieval-augmented LLM systems, we develop RETA-LLM, a {RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline to help researchers and users build their customized in-domain LLM-based systems. Compared with previous retrieval-augmented LLM systems, RETA-LLM provides more plug-and-play modules to support better interaction between IR systems and LLMs, including {request rewriting, document retrieval, passage extraction, answer generation, and fact checking} modules. Our toolkit is publicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.",
    "url": "http://arxiv.org/abs/2306.05212v1",
    "pdf_url": "https://arxiv.org/pdf/2306.05212v1",
    "published_date": "2023-06-08",
    "is_downloadable": true
  },
  {
    "source": "arxiv",
    "id": "2407.07093v1",
    "title": "FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation",
    "authors": [
      "Liqun Ma",
      "Mingjie Sun",
      "Zhiqiang Shen"
    ],
    "abstract": "This work presents a Fully BInarized Large Language Model (FBI-LLM), demonstrating for the first time how to train a large-scale binary language model from scratch (not the partial binary or ternary LLM like BitNet b1.58) to match the performance of its full-precision counterparts (e.g., FP16 or BF16) in transformer-based LLMs. It achieves this by employing an autoregressive distillation (AD) loss with maintaining equivalent model dimensions (130M, 1.3B, 7B) and training data volume as regular LLM pretraining, while delivering competitive results in terms of perplexity and task-specific effectiveness. Intriguingly, by analyzing the training trajectory, we find that the pretrained weight is not necessary for training binarized LLMs from scratch. This research encourages a new computational framework and may facilitate the future design of specialized hardware tailored for fully 1-bit LLMs. We make all models, code, and training dataset fully accessible and transparent to support further research (Code: https://github.com/LiqunMa/FBI-LLM. Model: https://huggingface.co/LiqunMa/).",
    "url": "http://arxiv.org/abs/2407.07093v1",
    "pdf_url": "https://arxiv.org/pdf/2407.07093v1",
    "published_date": "2024-07-09",
    "is_downloadable": true
  }
]